{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
        "!unzip -q tiny-imagenet-200.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDbIaDa_D5HA",
        "outputId": "e52fb1f6-e878-4fae-9542-8e1649b3e58b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-19 10:51:23--  http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
            "Resolving cs231n.stanford.edu (cs231n.stanford.edu)... 171.64.64.64\n",
            "Connecting to cs231n.stanford.edu (cs231n.stanford.edu)|171.64.64.64|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://cs231n.stanford.edu/tiny-imagenet-200.zip [following]\n",
            "--2025-08-19 10:51:23--  https://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
            "Connecting to cs231n.stanford.edu (cs231n.stanford.edu)|171.64.64.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 248100043 (237M) [application/zip]\n",
            "Saving to: ‘tiny-imagenet-200.zip’\n",
            "\n",
            "tiny-imagenet-200.z 100%[===================>] 236.61M  47.7MB/s    in 5.0s    \n",
            "\n",
            "2025-08-19 10:51:28 (47.5 MB/s) - ‘tiny-imagenet-200.zip’ saved [248100043/248100043]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, math, random, time\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms, utils"
      ],
      "metadata": {
        "id": "SR0A9gtULloh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    HAS_SK = True\n",
        "except Exception:\n",
        "    HAS_SK = False"
      ],
      "metadata": {
        "id": "6UqjPt2HLn3I"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup the variable"
      ],
      "metadata": {
        "id": "u9ZxL9G7Oo9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "SEED = 42\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "AMP = True\n",
        "\n",
        "\n",
        "TRAIN_DIR = \"/content/tiny-imagenet-200/train\"\n",
        "\n",
        "NUM_CLASSES = 20\n",
        "IMG_SIZE = 64\n",
        "VAL_SPLIT = 0.15\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 20\n",
        "LR = 3e-4\n",
        "WEIGHT_DECAY = 0.05\n",
        "EARLY_STOP = 6\n",
        "\n",
        "OUT_DIR = Path(\"tri_compare_vit\")\n",
        "(OUT_DIR / \"figs\").mkdir(parents=True, exist_ok=True)\n",
        "(OUT_DIR / \"grids\").mkdir(parents=True, exist_ok=True)\n",
        "(OUT_DIR / \"curves\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "random.seed(SEED); np.random.seed(SEED)\n",
        "torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)"
      ],
      "metadata": {
        "id": "WN4r7d8NLp9Q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data & selection of 20 classes"
      ],
      "metadata": {
        "id": "x6-xvP7sO2Hv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_tfms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(0.2,0.2,0.2,0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.4802,0.4481,0.3975), std=(0.2770,0.2691,0.2821)),\n",
        "])\n",
        "\n",
        "eval_tfms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.4802,0.4481,0.3975), std=(0.2770,0.2691,0.2821)),\n",
        "])"
      ],
      "metadata": {
        "id": "UFCKFeweLwPB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_ds = datasets.ImageFolder(TRAIN_DIR)\n",
        "all_classes = full_ds.classes\n",
        "print(f\"Found {len(all_classes)} classes; selecting {NUM_CLASSES}…\")\n",
        "rng = random.Random(SEED)\n",
        "selected_classes = sorted(rng.sample(all_classes, NUM_CLASSES))\n",
        "print(\"Selected classes:\", selected_classes)\n",
        "\n",
        "sel_to_new = {c:i for i,c in enumerate(selected_classes)}\n",
        "\n",
        "sel_indices = [i for i,(_,y) in enumerate(full_ds.samples) if full_ds.classes[y] in sel_to_new]"
      ],
      "metadata": {
        "id": "y5SYXnibLyzg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e90fbcf-1357-474b-d9a9-d630ad8ac4dc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 200 classes; selecting 20…\n",
            "Selected classes: ['n01768244', 'n01770393', 'n01774384', 'n02058221', 'n02074367', 'n02099601', 'n02106662', 'n02132136', 'n02481823', 'n02504458', 'n02666196', 'n02730930', 'n02814533', 'n03447447', 'n04067472', 'n04265275', 'n04456115', 'n04562935', 'n07753592', 'n07768694']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class WrappedSubset(torch.utils.data.Dataset):\n",
        "    def __init__(self, base, indices, transform, sel_to_new):\n",
        "        self.base = base\n",
        "        self.indices = indices\n",
        "        self.transform = transform\n",
        "        self.sel_to_new = sel_to_new\n",
        "\n",
        "    def __len__(self): return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        bi = self.indices[i]\n",
        "        path, y_old = self.base.samples[bi]\n",
        "        img = self.base.loader(path)\n",
        "        img = self.transform(img)\n",
        "        y = self.sel_to_new[self.base.classes[y_old]]\n",
        "        return img, y"
      ],
      "metadata": {
        "id": "UQzWli8TL2fY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "N = len(sel_indices)\n",
        "n_val = int(round(VAL_SPLIT * N))\n",
        "n_train = N - n_val\n",
        "train_ids, val_ids = random_split(sel_indices, [n_train, n_val], generator=torch.Generator().manual_seed(SEED))\n",
        "\n",
        "train_ds = WrappedSubset(full_ds, list(train_ids), transform=train_tfms, sel_to_new=sel_to_new)\n",
        "val_ds   = WrappedSubset(full_ds, list(val_ids),  transform=eval_tfms,  sel_to_new=sel_to_new)\n",
        "\n",
        "num_workers = min(8, os.cpu_count() or 2)\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=num_workers, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)"
      ],
      "metadata": {
        "id": "1ZFtH14ML45A"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### print image for each class"
      ],
      "metadata": {
        "id": "eHEBimkgPUSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def denorm(x):\n",
        "    mean = torch.tensor([0.4802, 0.4481, 0.3975])[:,None,None]\n",
        "    std  = torch.tensor([0.2770, 0.2691, 0.2821])[:,None,None]\n",
        "    return x*std + mean\n",
        "\n",
        "def save_example_grids_per_class(dataset, per_class=20):\n",
        "    per = {i:[] for i in range(NUM_CLASSES)}\n",
        "    for i in range(len(dataset)):\n",
        "        _, y = dataset[i]\n",
        "        if len(per[y]) < per_class:\n",
        "            per[y].append(i)\n",
        "    inv_map = {v:k for k,v in sel_to_new.items()}\n",
        "    for ci, idxs in per.items():\n",
        "        imgs = [dataset[j][0] for j in idxs]\n",
        "        if not imgs: continue\n",
        "        grid = utils.make_grid(imgs, nrow=5, padding=2)\n",
        "        fig = plt.figure(figsize=(6,5))\n",
        "        plt.imshow(np.transpose(denorm(grid).clamp(0,1).numpy(), (1,2,0)))\n",
        "        plt.axis(\"off\"); plt.title(f\"class {ci}: {inv_map[ci]}\")\n",
        "        p = OUT_DIR / \"grids\" / f\"class_{ci:02d}.png\"\n",
        "        fig.tight_layout(); fig.savefig(p, dpi=200); plt.close(fig)\n",
        "        print(\"Saved:\", p)\n",
        "\n",
        "print(\"Saving a 20-image grid for each class from the training set…\")\n",
        "save_example_grids_per_class(train_ds, per_class=20)"
      ],
      "metadata": {
        "id": "U2Cf9kspL-VB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e637fd6-7fbb-40de-8f9b-df449539e134"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving a 20-image grid for each class from the training set…\n",
            "Saved: tri_compare_vit/grids/class_00.png\n",
            "Saved: tri_compare_vit/grids/class_01.png\n",
            "Saved: tri_compare_vit/grids/class_02.png\n",
            "Saved: tri_compare_vit/grids/class_03.png\n",
            "Saved: tri_compare_vit/grids/class_04.png\n",
            "Saved: tri_compare_vit/grids/class_05.png\n",
            "Saved: tri_compare_vit/grids/class_06.png\n",
            "Saved: tri_compare_vit/grids/class_07.png\n",
            "Saved: tri_compare_vit/grids/class_08.png\n",
            "Saved: tri_compare_vit/grids/class_09.png\n",
            "Saved: tri_compare_vit/grids/class_10.png\n",
            "Saved: tri_compare_vit/grids/class_11.png\n",
            "Saved: tri_compare_vit/grids/class_12.png\n",
            "Saved: tri_compare_vit/grids/class_13.png\n",
            "Saved: tri_compare_vit/grids/class_14.png\n",
            "Saved: tri_compare_vit/grids/class_15.png\n",
            "Saved: tri_compare_vit/grids/class_16.png\n",
            "Saved: tri_compare_vit/grids/class_17.png\n",
            "Saved: tri_compare_vit/grids/class_18.png\n",
            "Saved: tri_compare_vit/grids/class_19.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def save_one_per_class_grid(dataset, out_path, classes=NUM_CLASSES, nrow=5):\n",
        "    found = {c: None for c in range(classes)}\n",
        "    images, labels = [], []\n",
        "    for i in range(len(dataset)):\n",
        "        x, y = dataset[i]\n",
        "        if found[y] is None:\n",
        "            found[y] = i\n",
        "            images.append(x); labels.append(y)\n",
        "        if len(images) == classes: break\n",
        "    if len(images) < classes:\n",
        "        need = [c for c, idx in found.items() if idx is None]\n",
        "        for j in range(len(val_ds)):\n",
        "            x, y = val_ds[j]\n",
        "            if y in need and all(l != y for l in labels):\n",
        "                images.append(x); labels.append(y)\n",
        "            if len(images) == classes: break\n",
        "    assert len(images) == classes, \"Could not find one sample for each class.\"\n",
        "    grid = utils.make_grid(images, nrow=nrow, padding=2)\n",
        "    fig = plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(np.transpose(denorm(grid).clamp(0,1).numpy(), (1,2,0)))\n",
        "    plt.axis(\"off\"); plt.title(\"One image per class (20 classes)\")\n",
        "    fig.tight_layout(); fig.savefig(out_path, dpi=220); plt.close(fig)\n",
        "    print(\"Saved:\", out_path)\n",
        "\n",
        "one_per_class_path = OUT_DIR / \"figs\" / \"one_per_class_20_grid.png\"\n",
        "save_one_per_class_grid(train_ds, one_per_class_path, classes=NUM_CLASSES, nrow=5)"
      ],
      "metadata": {
        "id": "zr-afHNaMCKo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f32e544f-69ab-4542-8f9a-aa7919eb10dc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: tri_compare_vit/figs/one_per_class_20_grid.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FCNN"
      ],
      "metadata": {
        "id": "B2LTyHIbMKXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class FCFNN(nn.Module):\n",
        "    def __init__(self, img_size=64, num_classes=20, widths=(1024,512)):\n",
        "        super().__init__()\n",
        "        C=3; H=W=img_size\n",
        "        flat = C*H*W\n",
        "        layers = [nn.Flatten()]\n",
        "        in_dim = flat\n",
        "        for w in widths:\n",
        "            layers += [nn.Linear(in_dim, w), nn.ReLU(inplace=True), nn.Dropout(0.2)]\n",
        "            in_dim = w\n",
        "        layers += [nn.Linear(in_dim, num_classes)]\n",
        "        self.net = nn.Sequential(*layers)\n",
        "    def forward(self, x): return self.net(x)"
      ],
      "metadata": {
        "id": "38aJmYj-MIfA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Small CNN\n"
      ],
      "metadata": {
        "id": "NTF3jGtLMPfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SmallCNN(nn.Module):\n",
        "    def __init__(self, num_classes=20):\n",
        "        super().__init__()\n",
        "        def block(cin, cout):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(cin, cout, 3, padding=1), nn.BatchNorm2d(cout), nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(cout, cout, 3, padding=1), nn.BatchNorm2d(cout), nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(2)\n",
        "            )\n",
        "        self.features = nn.Sequential(\n",
        "            block(3,   64),\n",
        "            block(64, 128),\n",
        "            block(128,256),\n",
        "        )\n",
        "        self.head = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        return self.head(x)"
      ],
      "metadata": {
        "id": "wO02PyLQMR2Y"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### All patchs"
      ],
      "metadata": {
        "id": "sOs01LxeMYsA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SinusoidalPositionalEmbedding(nn.Module):\n",
        "    def __init__(self, dim, max_len=1024):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, dim)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, dim, 2).float() * (-math.log(10000.0) / dim))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "    def forward(self, x):\n",
        "        L = x.size(1)\n",
        "        return x + self.pe[:, :L, :]\n",
        "\n",
        "class PatchifyConv(nn.Module):\n",
        "    def __init__(self, img_size=64, patch=8, in_ch=3, embed_dim=192):\n",
        "        super().__init__()\n",
        "        assert img_size % patch == 0\n",
        "        self.num_patches = (img_size // patch) ** 2\n",
        "        self.proj = nn.Conv2d(in_ch, embed_dim, kernel_size=patch, stride=patch)\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "class PatchifyLinear(nn.Module):\n",
        "    def __init__(self, img_size=64, patch=8, in_ch=3, embed_dim=192):\n",
        "        super().__init__()\n",
        "        assert img_size % patch == 0\n",
        "        self.patch = patch\n",
        "        self.num_patches = (img_size // patch) ** 2\n",
        "        self.proj = nn.Linear(in_ch*patch*patch, embed_dim)\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        p = self.patch\n",
        "        patches = F.unfold(x, kernel_size=p, stride=p)\n",
        "        patches = patches.transpose(1,2)\n",
        "        return self.proj(patches)"
      ],
      "metadata": {
        "id": "66kUg8neMbw4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer"
      ],
      "metadata": {
        "id": "E0XtcOcyMjJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, heads, mlp_ratio=4.0, attn_drop=0.0, proj_drop=0.0, drop=0.0):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn  = nn.MultiheadAttention(dim, heads, dropout=attn_drop, batch_first=True)\n",
        "        self.drop1 = nn.Dropout(drop)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.mlp   = nn.Sequential(\n",
        "            nn.Linear(dim, int(dim*mlp_ratio)),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(proj_drop),\n",
        "            nn.Linear(int(dim*mlp_ratio), dim),\n",
        "            nn.Dropout(proj_drop),\n",
        "        )\n",
        "        self.drop2 = nn.Dropout(drop)\n",
        "    def forward(self, x):\n",
        "        h = x\n",
        "        x = self.norm1(x)\n",
        "        x,_ = self.attn(x,x,x, need_weights=False)\n",
        "        x = h + self.drop1(x)\n",
        "        h = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.mlp(x)\n",
        "        x = h + self.drop2(x)\n",
        "        return x\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    def __init__(self,\n",
        "                 img_size=64, patch=8, in_ch=3, num_classes=20,\n",
        "                 embed_dim=192, depth=8, heads=6, mlp_ratio=4.0,\n",
        "                 patch_type=\"conv\",      # \"conv\" | \"linear\"\n",
        "                 pos_type=\"learnable\"    # \"learnable\" | \"sinusoidal\" | \"none\"\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        if patch_type == \"conv\":\n",
        "            self.patch = PatchifyConv(img_size, patch, in_ch, embed_dim)\n",
        "        elif patch_type == \"linear\":\n",
        "            self.patch = PatchifyLinear(img_size, patch, in_ch, embed_dim)\n",
        "        else:\n",
        "            raise ValueError(\"patch_type must be 'conv' or 'linear'.\")\n",
        "\n",
        "        num_patches = self.patch.num_patches\n",
        "        self.cls = nn.Parameter(torch.zeros(1,1,embed_dim))\n",
        "        self.pos_type = pos_type\n",
        "        if pos_type == \"learnable\":\n",
        "            self.pos = nn.Parameter(torch.zeros(1, 1+num_patches, embed_dim))\n",
        "            nn.init.trunc_normal_(self.pos, std=0.02)\n",
        "        elif pos_type == \"sinusoidal\":\n",
        "            self.pos = SinusoidalPositionalEmbedding(embed_dim, max_len=1+num_patches)\n",
        "        elif pos_type == \"none\":\n",
        "            self.pos = None\n",
        "        else:\n",
        "            raise ValueError(\"pos_type must be learnable/sinusoidal/none\")\n",
        "\n",
        "        self.blocks = nn.ModuleList([TransformerBlock(embed_dim, heads, mlp_ratio) for _ in range(depth)])\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "        nn.init.trunc_normal_(self.cls, std=0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.size(0)\n",
        "        x = self.patch(x)                      # (B, N, E)\n",
        "        cls = self.cls.expand(B,-1,-1)         # (B,1,E)\n",
        "        x = torch.cat([cls, x], dim=1)         # (B,1+N,E)\n",
        "        if self.pos_type == \"learnable\":\n",
        "            x = x + self.pos[:, :x.size(1), :]\n",
        "        elif self.pos_type == \"sinusoidal\":\n",
        "            x = self.pos(x)\n",
        "        for blk in self.blocks: x = blk(x)\n",
        "        x = self.norm(x)[:,0]\n",
        "        return self.head(x)"
      ],
      "metadata": {
        "id": "KRyApnTdMlmw"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and Evaluation"
      ],
      "metadata": {
        "id": "aPmQl6ggMoD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def top1(logits, y):\n",
        "    return (logits.argmax(1) == y).float().mean().item()\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    tot_loss=tot_acc=0.0; n=0\n",
        "    all_logits=[]; all_y=[]\n",
        "    for x,y in loader:\n",
        "        x=x.to(device, non_blocking=True); y=y.to(device, non_blocking=True)\n",
        "        logits = model(x)\n",
        "        loss = F.cross_entropy(logits, y)\n",
        "        bs = x.size(0)\n",
        "        tot_loss += loss.item()*bs\n",
        "        tot_acc  += top1(logits, y)*bs\n",
        "        n += bs\n",
        "        all_logits.append(logits.cpu()); all_y.append(y.cpu())\n",
        "    return tot_loss/n, tot_acc/n, torch.cat(all_logits), torch.cat(all_y)\n",
        "\n",
        "def train_model(model, train_loader, val_loader, epochs=EPOCHS, lr=LR, wd=WEIGHT_DECAY, device=DEVICE, early_stop=EARLY_STOP, run_name=\"run\"):\n",
        "    model.to(device)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=(AMP and device.startswith(\"cuda\")))\n",
        "\n",
        "    history = {\"train_loss\":[], \"train_acc\":[], \"val_loss\":[], \"val_acc\":[]}\n",
        "    best = float(\"inf\"); best_state=None; wait=0\n",
        "\n",
        "    for ep in range(1, epochs+1):\n",
        "        model.train()\n",
        "        tr_loss=tr_acc=0.0; n=0\n",
        "        t0=time.time()\n",
        "        for x,y in train_loader:\n",
        "            x=x.to(device, non_blocking=True); y=y.to(device, non_blocking=True)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with torch.cuda.amp.autocast(enabled=(AMP and device.startswith(\"cuda\"))):\n",
        "                logits = model(x)\n",
        "                loss = F.cross_entropy(logits, y)\n",
        "            scaler.scale(loss).backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            scaler.step(opt); scaler.update()\n",
        "\n",
        "            bs=x.size(0)\n",
        "            tr_loss += loss.item()*bs\n",
        "            tr_acc  += top1(logits, y)*bs\n",
        "            n += bs\n",
        "\n",
        "        tr_loss/=n; tr_acc/=n\n",
        "        val_loss, val_acc, _, _ = evaluate(model, val_loader, device)\n",
        "        sch.step()\n",
        "\n",
        "        history[\"train_loss\"].append(tr_loss)\n",
        "        history[\"train_acc\"].append(tr_acc)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "\n",
        "        print(f\"[{run_name}] Epoch {ep:02d}: \"\n",
        "              f\"train_loss={tr_loss:.4f} acc={tr_acc:.4f} | \"\n",
        "              f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | \"\n",
        "              f\"lr={sch.get_last_lr()[0]:.2e} time={time.time()-t0:.1f}s\")\n",
        "\n",
        "        if val_loss < best - 1e-4:\n",
        "            best = val_loss\n",
        "            best_state = {k:v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
        "            wait=0\n",
        "        else:\n",
        "            wait+=1\n",
        "            if wait>=early_stop:\n",
        "                print(f\"[{run_name}] Early stop.\")\n",
        "                break\n",
        "\n",
        "    if best_state is not None: model.load_state_dict(best_state)\n",
        "    return model, history"
      ],
      "metadata": {
        "id": "PTKimvnuMycg"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot curves"
      ],
      "metadata": {
        "id": "d0pxkCNAM0AQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_curves(history, title, out_png):\n",
        "    ep = range(1, len(history[\"train_loss\"])+1)\n",
        "    plt.figure(figsize=(7.5,4))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(ep, history[\"train_loss\"], label=\"Train\")\n",
        "    plt.plot(ep, history[\"val_loss\"],   label=\"Val\")\n",
        "    plt.title(\"Loss\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend()\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(ep, history[\"train_acc\"], label=\"Train\")\n",
        "    plt.plot(ep, history[\"val_acc\"],   label=\"Val\")\n",
        "    plt.title(\"Accuracy\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Acc\"); plt.legend()\n",
        "    plt.suptitle(title); plt.tight_layout()\n",
        "    plt.savefig(out_png, dpi=220); plt.close()\n",
        "    print(\"Saved:\", out_png)"
      ],
      "metadata": {
        "id": "Aurz2uK4M3Wo"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiment Registry"
      ],
      "metadata": {
        "id": "jt9fQMpMNBGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class RunSpec:\n",
        "    family: str         # \"ViT\" | \"CNN\" | \"FCFNN\"\n",
        "    name: str           # short name used in plots\n",
        "    params: dict        # kwargs for model ctor\n",
        "\n",
        "def build_model(spec: RunSpec):\n",
        "    if spec.family == \"ViT\":\n",
        "        return ViT(num_classes=NUM_CLASSES, img_size=IMG_SIZE, **spec.params)\n",
        "    if spec.family == \"CNN\":\n",
        "        return SmallCNN(num_classes=NUM_CLASSES)\n",
        "    if spec.family == \"FCFNN\":\n",
        "        return FCFNN(img_size=IMG_SIZE, num_classes=NUM_CLASSES, **spec.params)\n",
        "    raise ValueError(\"Unknown family\")\n",
        "\n",
        "# Baselines\n",
        "runs = [\n",
        "    RunSpec(\"FCFNN\", \"FCFNN\", {\"widths\": (1024,512)}),\n",
        "    RunSpec(\"CNN\",   \"SmallCNN\", {}),\n",
        "    RunSpec(\"ViT\",   \"ViT(h=4,Conv,LearnPE)\", {\"patch\":8, \"embed_dim\":192, \"depth\":8, \"heads\":4, \"patch_type\":\"conv\", \"pos_type\":\"learnable\"}),\n",
        "]\n",
        "\n",
        "# ViT ablation: number of heads\n",
        "for h in [2,4,6,8]:\n",
        "    runs.append(RunSpec(\"ViT\", f\"ViT_heads={h}\", {\"patch\":8,\"embed_dim\":192,\"depth\":6,\"heads\":h,\"patch_type\":\"conv\",\"pos_type\":\"learnable\"}))\n",
        "\n",
        "# ViT ablation: patch embedding type\n",
        "runs += [\n",
        "    RunSpec(\"ViT\", \"ViT_PatchConv\",   {\"patch\":8,\"embed_dim\":192,\"depth\":6,\"heads\":4,\"patch_type\":\"conv\",\"pos_type\":\"learnable\"}),\n",
        "    RunSpec(\"ViT\", \"ViT_PatchLinear\", {\"patch\":8,\"embed_dim\":192,\"depth\":6,\"heads\":4,\"patch_type\":\"linear\",\"pos_type\":\"learnable\"}),\n",
        "]\n",
        "\n",
        "# ViT ablation: positional embedding type\n",
        "runs += [\n",
        "    RunSpec(\"ViT\", \"ViT_PosLearn\", {\"patch\":8,\"embed_dim\":192,\"depth\":6,\"heads\":4,\"patch_type\":\"conv\",\"pos_type\":\"learnable\"}),\n",
        "    RunSpec(\"ViT\", \"ViT_PosSine\",  {\"patch\":8,\"embed_dim\":192,\"depth\":6,\"heads\":4,\"patch_type\":\"conv\",\"pos_type\":\"sinusoidal\"}),\n",
        "    RunSpec(\"ViT\", \"ViT_PosNone\",  {\"patch\":8,\"embed_dim\":192,\"depth\":6,\"heads\":4,\"patch_type\":\"conv\",\"pos_type\":\"none\"}),\n",
        "]"
      ],
      "metadata": {
        "id": "IDadDKRiNDtI"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run all experiments"
      ],
      "metadata": {
        "id": "4igh5rtPNQQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "histories = {}\n",
        "results = []\n",
        "\n",
        "for spec in runs:\n",
        "    run_name = spec.name\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"Training:\", run_name, \"|\", spec.family, spec.params)\n",
        "    model = build_model(spec)\n",
        "    model, hist = train_model(model, train_loader, val_loader, run_name=run_name)\n",
        "    histories[run_name] = hist\n",
        "\n",
        "    curve_path = OUT_DIR / \"curves\" / f\"{run_name}.png\"\n",
        "    plot_curves(hist, title=run_name, out_png=curve_path)\n",
        "\n",
        "    val_loss, val_acc, logits, tgts = evaluate(model, val_loader, DEVICE)\n",
        "    rec = {\"name\": run_name, \"family\": spec.family, \"val_loss\": val_loss, \"val_acc\": val_acc}\n",
        "    rec.update({f\"p:{k}\":v for k,v in spec.params.items()})\n",
        "    results.append(rec)\n",
        "\n",
        "    if HAS_SK and spec.family in [\"ViT\",\"CNN\"]:\n",
        "        cm = confusion_matrix(tgts.numpy(), logits.argmax(1).numpy(), labels=list(range(NUM_CLASSES)))\n",
        "        plt.figure(figsize=(6,5))\n",
        "        plt.imshow(cm, interpolation='nearest'); plt.colorbar()\n",
        "        plt.title(f\"Confusion Matrix — {run_name}\")\n",
        "        plt.xlabel(\"Pred\"); plt.ylabel(\"True\"); plt.tight_layout()\n",
        "        p = OUT_DIR / \"figs\" / f\"cm_{run_name}.png\"\n",
        "        plt.savefig(p, dpi=200); plt.close()\n",
        "        print(\"Saved:\", p)"
      ],
      "metadata": {
        "id": "n4yJ1zRwNYgo",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summaries & Standard Comparison Plots"
      ],
      "metadata": {
        "id": "1sd_2U5KNo_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(results)\n",
        "csv_path = OUT_DIR / \"summary.csv\"\n",
        "df.to_csv(csv_path, index=False)\n",
        "print(\"\\nSaved summary:\", csv_path)\n",
        "print(df.sort_values(\"val_acc\", ascending=False)[[\"name\",\"family\",\"val_acc\",\"val_loss\"]].to_string(index=False))\n",
        "\n",
        "best_per_family = df.sort_values(\"val_acc\", ascending=False).groupby(\"family\").head(1)\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.bar(best_per_family[\"family\"], best_per_family[\"val_acc\"])\n",
        "for x, a in zip(best_per_family[\"family\"], best_per_family[\"val_acc\"]):\n",
        "    plt.text(x, a+0.002, f\"{a:.3f}\", ha=\"center\", va=\"bottom\")\n",
        "plt.title(\"Best Validation Accuracy per Family\"); plt.ylabel(\"Val Acc\"); plt.tight_layout()\n",
        "p = OUT_DIR / \"figs\" / \"best_per_family.png\"\n",
        "plt.savefig(p, dpi=220); plt.close(); print(\"Saved:\", p)\n",
        "\n",
        "vit_heads = df[df[\"name\"].str.startswith(\"ViT_heads\")]\n",
        "if not vit_heads.empty:\n",
        "    vit_heads_sorted = vit_heads.sort_values(\"p:heads\")\n",
        "    plt.figure(figsize=(7,4))\n",
        "    xs = [str(h) for h in vit_heads_sorted[\"p:heads\"]]\n",
        "    plt.bar(xs, vit_heads_sorted[\"val_acc\"])\n",
        "    for x,a in zip(xs, vit_heads_sorted[\"val_acc\"]):\n",
        "        plt.text(x, a+0.002, f\"{a:.3f}\", ha=\"center\", va=\"bottom\")\n",
        "    plt.xlabel(\"#Heads\"); plt.ylabel(\"Val Acc\")\n",
        "    plt.title(\"Effect of Number of Heads (ViT)\"); plt.tight_layout()\n",
        "    p = OUT_DIR / \"figs\" / \"vit_heads_ablation.png\"\n",
        "    plt.savefig(p, dpi=220); plt.close(); print(\"Saved:\", p)\n",
        "\n",
        "vit_patch = df[df[\"name\"].str.startswith(\"ViT_Patch\")]\n",
        "if len(vit_patch) >= 2:\n",
        "    vit_patch = vit_patch.sort_values(\"name\")\n",
        "    plt.figure(figsize=(7,4))\n",
        "    xs = vit_patch[\"name\"].tolist()\n",
        "    plt.bar(xs, vit_patch[\"val_acc\"])\n",
        "    for x,a in zip(xs, vit_patch[\"val_acc\"]):\n",
        "        plt.text(x, a+0.002, f\"{a:.3f}\", ha=\"center\", va=\"bottom\")\n",
        "    plt.title(\"Effect of Patch Embedding (ViT)\"); plt.ylabel(\"Val Acc\")\n",
        "    plt.xticks(rotation=15); plt.tight_layout()\n",
        "    p = OUT_DIR / \"figs\" / \"vit_patch_ablation.png\"\n",
        "    plt.savefig(p, dpi=220); plt.close(); print(\"Saved:\", p)\n",
        "\n",
        "vit_pos = df[df[\"name\"].str.startswith(\"ViT_Pos\")]\n",
        "if len(vit_pos) >= 2:\n",
        "    vit_pos = vit_pos.sort_values(\"name\")\n",
        "    plt.figure(figsize=(7.5,4))\n",
        "    xs = vit_pos[\"name\"].tolist()\n",
        "    plt.bar(xs, vit_pos[\"val_acc\"])\n",
        "    for x,a in zip(xs, vit_pos[\"val_acc\"]):\n",
        "        plt.text(x, a+0.002, f\"{a:.3f}\", ha=\"center\", va=\"bottom\")\n",
        "    plt.title(\"Effect of Positional Embedding (ViT)\"); plt.ylabel(\"Val Acc\")\n",
        "    plt.xticks(rotation=15); plt.tight_layout()\n",
        "    p = OUT_DIR / \"figs\" / \"vit_pos_ablation.png\"\n",
        "    plt.savefig(p, dpi=220); plt.close(); print(\"Saved:\", p)"
      ],
      "metadata": {
        "id": "WJQvrXioNuvA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f71843c-4bb4-4974-fbeb-d523aca52e49"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saved summary: tri_compare_vit/summary.csv\n",
            "                 name family  val_acc  val_loss\n",
            "             SmallCNN    CNN 0.663333  1.160207\n",
            "ViT(h=4,Conv,LearnPE)    ViT 0.530000  1.620059\n",
            "          ViT_PosSine    ViT 0.527333  1.581337\n",
            "          ViT_heads=2    ViT 0.518000  1.633329\n",
            "          ViT_heads=8    ViT 0.512667  1.590942\n",
            "          ViT_heads=6    ViT 0.512000  1.658118\n",
            "          ViT_PosNone    ViT 0.512000  1.677102\n",
            "          ViT_heads=4    ViT 0.506667  1.645499\n",
            "         ViT_PosLearn    ViT 0.500667  1.618436\n",
            "        ViT_PatchConv    ViT 0.500667  1.642643\n",
            "      ViT_PatchLinear    ViT 0.493333  1.673637\n",
            "                FCFNN  FCFNN 0.363333  2.159235\n",
            "Saved: tri_compare_vit/figs/best_per_family.png\n",
            "Saved: tri_compare_vit/figs/vit_heads_ablation.png\n",
            "Saved: tri_compare_vit/figs/vit_patch_ablation.png\n",
            "Saved: tri_compare_vit/figs/vit_pos_ablation.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###COMBINED TABLE + DASHBOARD + COMBINED CURVES + PDF REPORT\n"
      ],
      "metadata": {
        "id": "zGC5TYE9NziY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "FIG_DIR = OUT_DIR / \"figs\"\n",
        "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def save_combined_table(df, out_png):\n",
        "    display_df = df.copy()\n",
        "    display_df[\"val_acc\"] = display_df[\"val_acc\"].map(lambda v: f\"{v:.3f}\")\n",
        "    display_df[\"val_loss\"] = display_df[\"val_loss\"].map(lambda v: f\"{v:.3f}\")\n",
        "    for col in [\"p:heads\", \"p:patch_type\", \"p:pos_type\", \"p:embed_dim\", \"p:depth\", \"p:patch\"]:\n",
        "        if col not in display_df.columns:\n",
        "            display_df[col] = \"\"\n",
        "    tmp = df.copy()\n",
        "    tmp[\"__ord\"] = -tmp[\"val_acc\"]\n",
        "    ord_idx = tmp.sort_values([\"family\",\"__ord\"]).index\n",
        "    display_df = display_df.loc[ord_idx]\n",
        "    show_cols = [\"name\",\"family\",\"val_acc\",\"val_loss\",\"p:heads\",\"p:patch_type\",\"p:pos_type\",\"p:patch\",\"p:embed_dim\",\"p:depth\"]\n",
        "    display_df = display_df[show_cols].rename(columns={\n",
        "        \"name\":\"Run\",\"family\":\"Family\",\"val_acc\":\"Val Acc\",\"val_loss\":\"Val Loss\",\n",
        "        \"p:heads\":\"Heads\",\"p:patch_type\":\"Patch Emb\",\"p:pos_type\":\"Pos Emb\",\n",
        "        \"p:patch\":\"PatchSz\",\"p:embed_dim\":\"Embed\",\"p:depth\":\"Depth\",\n",
        "    })\n",
        "    rows = len(display_df)\n",
        "    fig_h = 1.0 + rows * 0.38\n",
        "    fig, ax = plt.subplots(figsize=(13, fig_h))\n",
        "    ax.axis(\"off\")\n",
        "    the_table = ax.table(\n",
        "        cellText=display_df.values,\n",
        "        colLabels=display_df.columns.tolist(),\n",
        "        cellLoc=\"center\",\n",
        "        loc=\"upper left\",\n",
        "    )\n",
        "    the_table.auto_set_font_size(False)\n",
        "    the_table.set_fontsize(9)\n",
        "    the_table.scale(1, 1.2)\n",
        "    ax.set_title(\"All Runs — Validation Results & Key Params\", pad=10, fontsize=12, weight=\"bold\")\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(out_png, dpi=240, bbox_inches=\"tight\")\n",
        "    plt.close(fig)\n",
        "    print(\"Saved:\", out_png)\n",
        "\n",
        "def save_dashboard(df, out_png):\n",
        "    fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
        "    best_per_family = df.sort_values(\"val_acc\", ascending=False).groupby(\"family\").head(1)\n",
        "    axs[0,0].bar(best_per_family[\"family\"], best_per_family[\"val_acc\"])\n",
        "    for x, a in zip(best_per_family[\"family\"], best_per_family[\"val_acc\"]):\n",
        "        axs[0,0].text(x, a+0.003, f\"{a:.3f}\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
        "    axs[0,0].set_title(\"Best Validation Accuracy per Family\"); axs[0,0].set_ylabel(\"Val Acc\")\n",
        "    axs[0,0].set_ylim(0, max(0.01 + best_per_family[\"val_acc\"].max(), 0.1))\n",
        "\n",
        "    vit_heads = df[df[\"name\"].str.startswith(\"ViT_heads\")]\n",
        "    axs[0,1].set_title(\"ViT — Effect of Number of Heads\")\n",
        "    if not vit_heads.empty:\n",
        "        vit_heads = vit_heads.sort_values(\"p:heads\")\n",
        "        xs = vit_heads[\"p:heads\"].astype(int).tolist()\n",
        "        axs[0,1].bar([str(x) for x in xs], vit_heads[\"val_acc\"])\n",
        "        for x, a in zip(xs, vit_heads[\"val_acc\"]):\n",
        "            axs[0,1].text(str(x), a+0.003, f\"{a:.3f}\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
        "        axs[0,1].set_xlabel(\"#Heads\"); axs[0,1].set_ylabel(\"Val Acc\")\n",
        "    else:\n",
        "        axs[0,1].text(0.5, 0.5, \"No heads sweep runs\", ha=\"center\", va=\"center\")\n",
        "        axs[0,1].set_xticks([]); axs[0,1].set_yticks([])\n",
        "\n",
        "    vit_patch = df[df[\"name\"].str.startswith(\"ViT_Patch\")].sort_values(\"name\")\n",
        "    axs[1,0].set_title(\"ViT — Patch Embedding Choice\")\n",
        "    if len(vit_patch) >= 1:\n",
        "        xs = vit_patch[\"name\"].tolist()\n",
        "        axs[1,0].bar(xs, vit_patch[\"val_acc\"])\n",
        "        for x, a in zip(xs, vit_patch[\"val_acc\"]):\n",
        "            axs[1,0].text(x, a+0.003, f\"{a:.3f}\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
        "        axs[1,0].set_ylabel(\"Val Acc\"); axs[1,0].tick_params(axis=\"x\", rotation=15)\n",
        "    else:\n",
        "        axs[1,0].text(0.5, 0.5, \"No patch ablation runs\", ha=\"center\", va=\"center\")\n",
        "        axs[1,0].set_xticks([]); axs[1,0].set_yticks([])\n",
        "\n",
        "    vit_pos = df[df[\"name\"].str.startswith(\"ViT_Pos\")].sort_values(\"name\")\n",
        "    axs[1,1].set_title(\"ViT — Positional Embedding Choice\")\n",
        "    if len(vit_pos) >= 1:\n",
        "        xs = vit_pos[\"name\"].tolist()\n",
        "        axs[1,1].bar(xs, vit_pos[\"val_acc\"])\n",
        "        for x, a in zip(xs, vit_pos[\"val_acc\"]):\n",
        "            axs[1,1].text(x, a+0.003, f\"{a:.3f}\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
        "        axs[1,1].set_ylabel(\"Val Acc\"); axs[1,1].tick_params(axis=\"x\", rotation=15)\n",
        "    else:\n",
        "        axs[1,1].text(0.5, 0.5, \"No positional ablation runs\", ha=\"center\", va=\"center\")\n",
        "        axs[1,1].set_xticks([]); axs[1,1].set_yticks([])\n",
        "    fig.tight_layout(); fig.savefig(out_png, dpi=240, bbox_inches=\"tight\"); plt.close(fig)\n",
        "    print(\"Saved:\", out_png)\n",
        "\n",
        "def plot_best_family_curves(df, histories_dict, out_png):\n",
        "    plt.figure(figsize=(10, 4.5))\n",
        "    ax1 = plt.subplot(1,2,1); ax1.set_title(\"Loss Curves — Best per Family\")\n",
        "    ax2 = plt.subplot(1,2,2); ax2.set_title(\"Accuracy Curves — Best per Family\")\n",
        "    fam_best = df.sort_values(\"val_acc\", ascending=False).groupby(\"family\").head(1)\n",
        "    colors = {\"FCFNN\":\"tab:orange\", \"CNN\":\"tab:green\", \"ViT\":\"tab:blue\"}\n",
        "    plotted = False\n",
        "    for _, row in fam_best.iterrows():\n",
        "        name = row[\"name\"]; fam = row[\"family\"]\n",
        "        if name not in histories_dict:\n",
        "            continue\n",
        "        H = histories_dict[name]; ep = range(1, len(H[\"train_loss\"])+1); c = colors.get(fam, None)\n",
        "        ax1.plot(ep, H[\"train_loss\"], label=f\"{fam}-{name} (train)\", linestyle=\"--\", color=c)\n",
        "        ax1.plot(ep, H[\"val_loss\"],   label=f\"{fam}-{name} (val)\",   linestyle=\"-\",  color=c)\n",
        "        ax2.plot(ep, H[\"train_acc\"],  label=f\"{fam}-{name} (train)\", linestyle=\"--\", color=c)\n",
        "        ax2.plot(ep, H[\"val_acc\"],    label=f\"{fam}-{name} (val)\",   linestyle=\"-\",  color=c)\n",
        "        plotted = True\n",
        "    for ax in (ax1, ax2):\n",
        "        ax.set_xlabel(\"Epoch\"); ax.grid(True, linestyle=\"--\", linewidth=0.5); ax.legend(fontsize=8)\n",
        "    if not plotted:\n",
        "        plt.clf()\n",
        "        fig = plt.figure(figsize=(6,2)); plt.axis(\"off\")\n",
        "        plt.text(0.5, 0.5, \"Histories not available\", ha=\"center\", va=\"center\")\n",
        "        fig.savefig(out_png, dpi=240, bbox_inches=\"tight\"); plt.close(fig); print(\"Saved:\", out_png);\n",
        "        return\n",
        "    plt.tight_layout(); plt.savefig(out_png, dpi=240, bbox_inches=\"tight\"); plt.close(); print(\"Saved:\", out_png)\n",
        "\n",
        "combined_table_png = FIG_DIR / \"combined_table.png\"\n",
        "dashboard_png      = FIG_DIR / \"dashboard.png\"\n",
        "best_curves_png    = FIG_DIR / \"best_family_curves.png\"\n",
        "\n",
        "save_combined_table(df, combined_table_png)\n",
        "save_dashboard(df, dashboard_png)\n",
        "plot_best_family_curves(df, histories, best_curves_png)\n",
        "\n",
        "pdf_path = FIG_DIR / \"report.pdf\"\n",
        "with PdfPages(pdf_path) as pdf:\n",
        "    for img_path in [combined_table_png, dashboard_png, best_curves_png]:\n",
        "        if os.path.exists(img_path):\n",
        "            fig = plt.figure(figsize=(11, 8.5))\n",
        "            plt.imshow(mpimg.imread(img_path)); plt.axis(\"off\")\n",
        "            pdf.savefig(fig, bbox_inches=\"tight\"); plt.close(fig)\n",
        "print(\"Saved PDF report:\", pdf_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fm9XTHa_YNC3",
        "outputId": "52160065-40ca-4482-a1f6-dba8f2ceda88"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: tri_compare_vit/figs/combined_table.png\n",
            "Saved: tri_compare_vit/figs/dashboard.png\n",
            "Saved: tri_compare_vit/figs/best_family_curves.png\n",
            "Saved PDF report: tri_compare_vit/figs/report.pdf\n"
          ]
        }
      ]
    }
  ]
}